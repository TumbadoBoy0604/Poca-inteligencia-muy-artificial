---
tags: [IA]
---

# Vecinos proximos
El método de vecinos próximos o los modelos no paramétricos funcionan mejor a bajas dimensiones

Imaginemos que tenemos en una sola dimension un "cuadrado" unitario, en este caso su "area" va a ser simplemente la longitud del segmento.
Podemos imaginar tambien un "circulo" unitario, que en este caso va a tener de area la longitud del segmento, por lo que vemos que son iguales.

Llamamos $s$ al area del cuadrado y $c$ al area del circulo, al sacar la division de $\frac{c}{s}$ podemos
observar que nos da 1.

Ahora para 2 dimensiones:
Hacemos un cuadrado unitario, en el cual su area es $L \dot L$ por lo que en este caso seria 1 
Ahora imaginamos un circulo unitario, en el cual su area es $\pi \dot r^2$



$y = h_{\theta}(x) = sign(w_1x_1 + w_2x_2 + w_0)$

$sign(x) = 1 if x >= 0 or -1 if x<0$

$\theta = (w_0, w_1,w_2) \in \R^3$

$E_{in} = \frac{1}{m} \sum^{m}_{i=1} loss(y^i, h(x^i))$

Si al conjunto de datos, le aplicamos un modelo super complejo que minimice al maximo el error, podemos caer
en lo que se le llama **sobre-aprendimiento** lo cual no es optimo para nuestro objetivo, el cual es hacer predicciones
sobre los datos que **NO** tenemos.

 $h^* \approx h$ 

* Problema de optimizacion
* Encontrar la $h^*$ equivale a encontrar el vector de parametros $\theta ^*$ tal que:
    $E_{in} (h^*) \approx 0$


## Desigualdad de Hoeffding
$Pr[|E_o(h^*) - E_i(h^*)| >= \epsilon <= 2 e^{-2\epsilon^2}]$

Donde $M$ es el numero de datos y \epsilon es la diferencia entre el error de muestra y el error fuera de muestra impuesto

Entonces, el planteamiento $E_o(h^*) \approx E_i(h^*)$ es PAC

## El problema con la desigualdad de Hoeffding

* Supongamos un problema de clasificacion binaria con 10 instancias en el conjunto de entrenamiento
* Algoritmo de aprendizaje: clasificar en forma aleatoria
* Cual es la probabilidad de clasificar las 10 instancias?
* Cual es la probabilidad de clasificar bien las 10 instancias en alguna iteracion, si el algoritmo se entrena con un maximo de 1000 epoch

## Traduciendo

$Pr[|E_o(h^*) - E_i(h^*)| >= \epsilon ] <= Pr[\cup_{h\in H} |E_0(h) - E_i(h)| >= \epsilon$

$Pr[|E_o(h^*) - E_i(h^*)| >= \epsilon ] <= 2N e^{-2\epsilon^2 M}$
